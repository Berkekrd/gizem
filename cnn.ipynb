{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a name='imports'>\n\n# 2 <span style='color:blue'>|</span> Importing & Setup","metadata":{}},{"cell_type":"code","source":"pip install --upgrade tensorflow tensorflow-io","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:29:32.136351Z","iopub.execute_input":"2023-12-06T22:29:32.136774Z","iopub.status.idle":"2023-12-06T22:30:47.677532Z","shell.execute_reply.started":"2023-12-06T22:29:32.136742Z","shell.execute_reply":"2023-12-06T22:30:47.673906Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General Imports\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport random\nimport os\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Building Model\nfrom keras.utils import plot_model\nfrom tensorflow.keras import models\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.optimizers import legacy\n\n# Training Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Data Processing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.preprocessing.image import load_img","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-06T22:30:59.567577Z","iopub.execute_input":"2023-12-06T22:30:59.569458Z","iopub.status.idle":"2023-12-06T22:30:59.598208Z","shell.execute_reply.started":"2023-12-06T22:30:59.569338Z","shell.execute_reply":"2023-12-06T22:30:59.594083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Global variables\nSAVE = False\nSEED = 111\n\n# Setting seed for consistent results\ntf.keras.utils.set_random_seed(SEED)\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# Data Visualization updates\n%config InlineBackend.figure_format = 'retina'\nplt.rcParams[\"figure.figsize\"] = (16, 10)\nplt.rcParams.update({'font.size': 14})\n\n# Data Classifications\nCLASS_TYPES = ['pituitary', 'notumor', 'meningioma', 'glioma']\nN_TYPES = len(CLASS_TYPES)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:31:10.282713Z","iopub.execute_input":"2023-12-06T22:31:10.284337Z","iopub.status.idle":"2023-12-06T22:31:10.298624Z","shell.execute_reply.started":"2023-12-06T22:31:10.284288Z","shell.execute_reply":"2023-12-06T22:31:10.296996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n***\n<a name='import_data'>\n    \n# 3 <span style='color:blue'>|</span> Importing Data","metadata":{}},{"cell_type":"code","source":"# Function for inporting data           \ndef get_data_labels(directory, shuffle=True, random_state=0):\n    \"\"\"\n    Function used for going into the main training directory\n    whose directory has sub-class-types.\n    \"\"\"\n    from sklearn.utils import shuffle\n    import os\n\n    # Lists to store data and labels\n    data_path = []\n    data_labels = []\n    \n    for label in os.listdir(directory):\n        label_dir = os.path.join(directory, label)\n\n        # Avoid MacOS storing path\n        if not os.path.isdir(label_dir):\n            continue\n\n        # Going into each folder and getting image path\n        for image in os.listdir(label_dir):\n            image_path = os.path.join(label_dir, image)\n            data_path.append(image_path)\n            data_labels.append(label)\n            \n    if shuffle:\n        data_path, data_labels = shuffle(data_path, data_labels, random_state=random_state)\n            \n    return data_path, data_labels\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:31:11.484679Z","iopub.execute_input":"2023-12-06T22:31:11.485113Z","iopub.status.idle":"2023-12-06T22:31:11.497358Z","shell.execute_reply.started":"2023-12-06T22:31:11.485082Z","shell.execute_reply":"2023-12-06T22:31:11.495748Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up file paths for training and testing\nUSER_PATH = r\"/kaggle/input/brain-tumor-mri-dataset\"\ntrain_dir = USER_PATH + r'/Training/'\ntest_dir = USER_PATH + r'/Testing/'\n\n# Getting data using above function\ntrain_paths, train_labels = get_data_labels(train_dir)\ntest_paths, test_labels = get_data_labels(test_dir)\n\n# Printing traing and testing sample sizes\nprint('Training')\nprint(f'Number of Paths: {len(train_paths)}')\nprint(f'Number of Labels: {len(train_labels)}')\nprint('\\nTesting')\nprint(f'Number of Paths: {len(test_paths)}')\nprint(f'Number of Labels: {len(test_labels)}')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:31:12.057178Z","iopub.execute_input":"2023-12-06T22:31:12.058201Z","iopub.status.idle":"2023-12-06T22:31:13.7854Z","shell.execute_reply.started":"2023-12-06T22:31:12.058155Z","shell.execute_reply":"2023-12-06T22:31:13.784085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n\n<a name='vis'>\n    \n# 4 <span style='color:blue'>|</span> Data Visualization\n    \n## <b> 4.2 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'>  Data Distributions <b>","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(ncols=3, figsize=(20, 14))\n\n# Plotting training data types\nclass_counts = [len([x for x in train_labels if x == label]) for label in CLASS_TYPES]\nprint('Training Counts')\nprint(dict(zip(CLASS_TYPES, class_counts)))\n\nax[0].set_title('Training Data')\nax[0].pie(\n    class_counts,\n    labels=[label.title() for label in CLASS_TYPES],\n    colors=['#FAC500','#0BFA00', '#0066FA','#FA0000'], \n    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum(class_counts) / 100),\n    explode=tuple(0.01 for i in range(N_TYPES)),\n    textprops={'fontsize': 20}\n)\n\n# Plotting distribution of train test split\nax[1].set_title('Train Test Split')\nax[1].pie(\n    [len(train_labels), len(test_labels)],\n    labels=['Train','Test'],\n    colors=['darkcyan', 'orange'], \n    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum([len(train_labels), len(test_labels)]) / 100),\n    explode=(0.1, 0),\n    startangle=85,\n    textprops={'fontsize': 20}\n)\n\n# Plotting testing data types\nclass_counts = [len([x for x in test_labels if x == label]) for label in CLASS_TYPES]\nprint('\\nTesting Counts')\nprint(dict(zip(CLASS_TYPES, class_counts)))\n\nax[2].set_title('Testing Data')\nax[2].pie(\n    class_counts,\n    labels=[label.title() for label in CLASS_TYPES],\n    colors=['#FAC500', '#0BFA00', '#0066FA', '#FA0000'],\n    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum(class_counts) / 100),\n    explode=tuple(0.01 for i in range(N_TYPES)),  # Explode the slices slightly for better visualization\n    textprops={'fontsize': 20}  # Set the font size for the text on the pie chart\n)\n\n\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T22:31:14.878118Z","iopub.execute_input":"2023-12-06T22:31:14.878505Z","iopub.status.idle":"2023-12-06T22:31:15.829337Z","shell.execute_reply.started":"2023-12-06T22:31:14.878474Z","shell.execute_reply":"2023-12-06T22:31:15.828537Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A balanced distribution of categories in our training data is crucial for optimal model learning. It allows for comprehensive understanding of each category's characteristics, prevents biases, enhances generalization, and enables iterative refinement, leading to improved performance in accurately categorizing new data.\n\nWe also have a nice split percentage in our training and testing set.","metadata":{}},{"cell_type":"code","source":"# getting image to test output\nim = load_img(train_paths[3], target_size=(150, 150))\nim = img_to_array(im)\n\n# Reshape it to (1, 150, 150, 3)\nim = np.expand_dims(im, axis=0)\nprint(f'x reshaped: {im.shape}')\n\n# normilzation tensor\nim /= np.max(im) # ~ np.max(img_tensor)\n\n# Convert the array back to the image format\nim = array_to_img(im[0])\ndisplay(im)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:31:27.662083Z","iopub.execute_input":"2023-12-06T22:31:27.662511Z","iopub.status.idle":"2023-12-06T22:31:27.704244Z","shell.execute_reply.started":"2023-12-06T22:31:27.662465Z","shell.execute_reply":"2023-12-06T22:31:27.702811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display a list of images based on the given index\ndef show_images(paths, label_paths, index_list=range(10), im_size=250, figsize=(12, 8), save=False):\n    \"\"\"\n    Show images from a given path based on the inputted\n    list indices related to the desired images one wishes\n    to see.\n    \"\"\"\n\n    num_images = len(index_list)\n    num_rows = (num_images + 3) // 4\n    \n    _, ax = plt.subplots(nrows=num_rows, ncols=4, figsize=figsize)\n    ax = ax.flatten()\n\n    for i, index in enumerate(index_list):\n        if i >= num_images:\n            break\n        \n        image = load_img(paths[index], target_size=(im_size, im_size))\n        ax[i].imshow(image)\n        ax[i].set_title(f'{index}: {label_paths[index]}')\n        ax[i].axis('off')\n\n    plt.tight_layout()\n    \n    if save:\n        plt.savefig('show_image.pdf')\n    else:\n        plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:31:29.103393Z","iopub.execute_input":"2023-12-06T22:31:29.103797Z","iopub.status.idle":"2023-12-06T22:31:29.114178Z","shell.execute_reply.started":"2023-12-06T22:31:29.103765Z","shell.execute_reply":"2023-12-06T22:31:29.112579Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Four different data classification images, from three different angles (images are independent)\nshow_images(train_paths, train_labels, im_size=350, figsize=(13,10),\n            index_list=[0, 94, 235, 17,\n                        61, 324, 55, 45,\n                        374, 65, 391, 488])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:39.389438Z","iopub.execute_input":"2023-12-06T22:51:39.389883Z","iopub.status.idle":"2023-12-06T22:51:42.515646Z","shell.execute_reply.started":"2023-12-06T22:51:39.389849Z","shell.execute_reply":"2023-12-06T22:51:42.514671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n<a name='DP'>\n    \n# 5 <span style='color:blue'>|</span> Data Processing & Training Setup Values ","metadata":{}},{"cell_type":"code","source":"# Image size\nimage_size = (150, 150)\n\n# Training batch size\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:51.527934Z","iopub.execute_input":"2023-12-06T22:51:51.528397Z","iopub.status.idle":"2023-12-06T22:51:51.534216Z","shell.execute_reply.started":"2023-12-06T22:51:51.528362Z","shell.execute_reply":"2023-12-06T22:51:51.532947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data augmentation and preprocessing\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   rotation_range=10,\n                                   brightness_range=(0.85, 1.15),\n                                   width_shift_range=0.002,\n                                   height_shift_range=0.002,\n                                   shear_range=12.5,\n                                   zoom_range=0,\n                                   horizontal_flip=True,\n                                   vertical_flip=False,\n                                   fill_mode=\"nearest\")\n\n\n# applying the generator to training data with constant seed\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size=image_size,\n                                                    batch_size=batch_size,\n                                                    class_mode=\"categorical\",\n                                                    seed=SEED)\n\n# No augmentation of the test data, just rescaling\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# applying the generator to testing data with constant seed\ntest_generator = test_datagen.flow_from_directory(test_dir,\n                                                  target_size=image_size,\n                                                  batch_size=batch_size,\n                                                  class_mode=\"categorical\",\n                                                  shuffle=False,\n                                                  seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:51.953461Z","iopub.execute_input":"2023-12-06T22:51:51.953901Z","iopub.status.idle":"2023-12-06T22:51:57.99092Z","shell.execute_reply.started":"2023-12-06T22:51:51.953865Z","shell.execute_reply":"2023-12-06T22:51:57.989987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> 5.1.1 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'> Data Augmentation Class Indices <b>","metadata":{}},{"cell_type":"code","source":"# Accessing class indices for training data generator\nclass_indices_train = train_generator.class_indices\nclass_indices_train_list = list(train_generator.class_indices.keys())\n\n\n# Displaying categorical types\nprint(\"Categorical types for the training data:\")\nprint(class_indices_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:57.993169Z","iopub.execute_input":"2023-12-06T22:51:57.994195Z","iopub.status.idle":"2023-12-06T22:51:58.001483Z","shell.execute_reply.started":"2023-12-06T22:51:57.994157Z","shell.execute_reply":"2023-12-06T22:51:57.999977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> 5.1.2 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'>  Showing Data Augmentation <b>","metadata":{}},{"cell_type":"code","source":"def show_ImageDataGenerator(ImageDataGenerator, num_samples=5, figsize=(12, 12), save=False):\n    \"\"\"\n    Function to viusalize how the ImageDataGenerator augments the data\n    \"\"\"\n    \n    # Generate augmented samples\n    augmented_samples = train_generator.next()\n\n    # Extract images from the batch\n    images = augmented_samples[0][:num_samples]\n\n    # Display the augmented images\n    fig, axes = plt.subplots(1, num_samples, figsize=figsize)\n    \n    for i, ax in enumerate(axes):\n        ax.imshow(images[i])\n        ax.axis('off')\n        \n    plt.tight_layout()\n        \n    if save:\n        plt.savefig('show_ImageDataGenerator.pdf')\n        \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:58.003695Z","iopub.execute_input":"2023-12-06T22:51:58.004168Z","iopub.status.idle":"2023-12-06T22:51:58.014686Z","shell.execute_reply.started":"2023-12-06T22:51:58.004134Z","shell.execute_reply":"2023-12-06T22:51:58.013599Z"},"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_ImageDataGenerator(train_datagen, num_samples=5, figsize=(12.5, 8), save=SAVE)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:58.016991Z","iopub.execute_input":"2023-12-06T22:51:58.017703Z","iopub.status.idle":"2023-12-06T22:51:59.353835Z","shell.execute_reply.started":"2023-12-06T22:51:58.017667Z","shell.execute_reply":"2023-12-06T22:51:59.352432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> 5.2 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'> Training Setup Values <b>","metadata":{}},{"cell_type":"code","source":"# Image shape: height, width, RBG\nimage_shape = (image_size[0], image_size[1], 3)\n\n# Training epochs\nepochs = 40\n\n# Steps per epoch\nsteps_per_epoch = train_generator.samples // batch_size\n\n# Validation steps\nvalidation_steps = test_generator.samples // batch_size\n\nprint(f'Image shape: {image_shape}')\nprint(f'Epochs: {epochs}')\nprint(f'Batch size: {batch_size}')\nprint(f'Steps Per Epoch: {steps_per_epoch}')\nprint(f'Validation steps: {validation_steps}')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:51:59.355397Z","iopub.execute_input":"2023-12-06T22:51:59.355904Z","iopub.status.idle":"2023-12-06T22:51:59.366428Z","shell.execute_reply.started":"2023-12-06T22:51:59.355852Z","shell.execute_reply":"2023-12-06T22:51:59.364825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì #\n# Output Images and Labels Visualization #\n# ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì #\ndef plot_sample_predictions(model, test_generator, categories, test_dir, num_samples=9, figsize=(12, 8)):\n    \"\"\"\n    Nice display of prediction samples to see CNN predictions\n    for classification.\n    \"\"\"\n    # Make predictions on the test dataset\n    predictions = model.predict(test_generator)\n    predicted_categories = np.argmax(predictions, axis=1)\n    true_categories = test_generator.classes\n\n    # Randomly sample test images\n    test_images = np.array(test_generator.filepaths)\n    sample_indices = np.random.choice(len(test_images), size=num_samples, replace=False)\n    sample_images = test_images[sample_indices]\n    sample_predictions = [categories[predicted_categories[i]] for i in sample_indices]\n    sample_true_labels = [categories[true_categories[i]] for i in sample_indices]\n\n    # Plot sample images with their predicted and true labels\n    plt.figure(figsize=figsize)\n    \n    # Loop over samples\n    for i, image_path in enumerate(sample_images):\n        # Form subplot and plot\n        plt.subplot(3, 3, i + 1)\n        img = plt.imread(image_path)\n        plt.imshow(img)\n        plt.axis(\"off\")\n        \n        # Set axis label color depending on correct prediction or not\n        prediction_color = 'green' if sample_predictions[i] == sample_true_labels[i] else 'red'\n        plt.title(f\"Predicted: {sample_predictions[i]}\\nTrue: {sample_true_labels[i]}\", color=prediction_color)\n        \n    plt.tight_layout()\n    plt.show()\n\n    \n# ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì #\n#            Confusion matrix            #\n# ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì #\ndef CM(CNN_model, test_generator, categories):\n    \"\"\"\n    Function to return the confusion matrix of a given CNN model.\n    \"\"\"\n    from sklearn.metrics import confusion_matrix\n    # Predictions on test dataset\n    predictions = CNN_model.predict(test_generator)\n    predicted_categories = np.argmax(predictions, axis=1)\n    true_categories = test_generator.classes\n\n    # Create a confusion matrix\n    confusion_matrix_array = confusion_matrix(true_categories, predicted_categories)\n    \n    return confusion_matrix_array\n\n\n# ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì #\n#             Metric Analysis            #\n# ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì #\ndef calculate_metrics(confusion_matrix, categories):\n    \"\"\"\n    Function to calculate important metrics for multi-classification problems.\n    \"\"\"\n    # Calculating 4 different metrics\n    precision = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0)\n    recall = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    accuracy = np.sum(np.diag(confusion_matrix)) / np.sum(confusion_matrix)\n\n    # Printing the results based on each category\n    for i, category in enumerate(categories):\n        print(f\"Class: {category.title()}\")\n        print(f\"Precision: {precision[i]:.3f}\")\n        print(f\"Recall: {recall[i]:.3f}\")\n        print(f\"F1-Score: {f1_score[i]:.3f}\\n\")\n        \n    # Showing the total accuracy of the model\n    print(f\"\\nAccuracy: {accuracy:.3f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:52:02.801166Z","iopub.execute_input":"2023-12-06T22:52:02.801622Z","iopub.status.idle":"2023-12-06T22:52:02.820819Z","shell.execute_reply.started":"2023-12-06T22:52:02.801588Z","shell.execute_reply":"2023-12-06T22:52:02.819602Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n<a name='CNN'>\n    \n# 7 <span style='color:blue'>|</span> Initial CNN Model Tests\n\n    From `model_1` I tested the following tuples and got the following test accuarcies. Since parameters `3.` rounded to two decimal places performed the best, or equally the best, we will use it as we improve our model.\n1. filter size of `filter_size = (4, 4)` and `pool_size = (2, 2)` the accuaracy was `Test Accuracy: 0.97`\n2. filter size of `filter_size = (3, 3)` and `pool_size = (3, 3)` the accuaracy was `Test Accuracy: 0.97`\n3. filter size of `filter_size = (4, 4)` and `pool_size = (3, 3)` the accuaracy was `Test Accuracy: 0.98`\n4. filter size of `filter_size = (3, 3)` and `pool_size = (2, 2)` the accuaracy was `Test Accuracy: 0.98`\n    \nModel `3.` was the tested by switched parts, then all to `model.add(AveragePooling2D(pool_size=(3, 3)))`.\n- The results were good with a `Test Accuracy score: 0.9766`. But did not improve on upon the previous model.\n    \nModel `3.` was tested with different optimizers. The optimzer test accuracy scores were as follows.\n1. Adam: `Test Accuracy: 0.982`\n2. RMSprop: `Test Accuracy: 0.972`\n3. Nadam: `Test Accuracy: 0.964`\n\nIn addition various trials were done to the parameters of `ImageDataGenerator()` to help ensure that overfitting would be minimized.\n    \n***\n\n```python\n# Define the model architecture\nmodel_1 = models.Sequential()\n\n# Convolutional layer 1\nmodel_1.add(Conv2D(32, (4, 4), activation=\"relu\", input_shape=image_shape))\nmodel_1.add(MaxPooling2D(pool_size=(3, 3)))\n\n# Convolutional layer 2\nmodel_1.add(Conv2D(64, (4, 4), activation=\"relu\"))\nmodel_1.add(MaxPooling2D(pool_size=(3, 3)))\n\n# Convolutional layer 3\nmodel_1.add(Conv2D(128, (4, 4), activation=\"relu\"))\nmodel_1.add(MaxPooling2D(pool_size=(3, 3)))\n\n# Convolutional layer 4\nmodel_1.add(Conv2D(128, (4, 4), activation=\"relu\"))\nmodel_1.add(Flatten())\n\n# Full connect layers\nmodel_1.add(Dense(512, activation=\"relu\"))\nmodel_1.add(Dropout(0.5, seed=SEED))\nmodel_1.add(Dense(N_TYPES, activation=\"softmax\"))\n\nmodel_1.summary()\n\n```\n***\n\n- Convolutional layer 1:\n  - Input shape: (150, 150, 3)\n  - Number of filters: 32\n  - Filter size: (4, 4)\n  - Activation function: ReLU\n  - Output shape: (147, 147, 32) [Calculation: (150 - 4) + 1 = 147]\n\n- Max Pooling layer 1:\n  - Pool size: (3, 3)\n  - Output shape: (49, 49, 32) [Calculation: 147 / 3 = 49]\n\n- Convolutional layer 2:\n  - Input shape: (49, 49, 32)\n  - Number of filters: 64\n  - Filter size: (4, 4)\n  - Activation function: ReLU\n  - Output shape: (46, 46, 64) [Calculation: (49 - 4) + 1 = 46]\n\n- Max Pooling layer 2:\n  - Pool size: (3, 3)\n  - Output shape: (15, 15, 64) [Calculation: 46 / 3 = 15]\n\n- Convolutional layer 3:\n  - Input shape: (15, 15, 64)\n  - Number of filters: 128\n  - Filter size: (4, 4)\n  - Activation function: ReLU\n  - Output shape: (12, 12, 128) [Calculation: (15 - 4) + 1 = 12]\n\n- Max Pooling layer 3:\n  - Pool size: (3, 3)\n  - Output shape: (4, 4, 128) [Calculation: 12 / 3 = 4]\n\n- Convolutional layer 4:\n  - Input shape: (4, 4, 128)\n  - Number of filters: 128\n  - Filter size: (4, 4)\n  - Activation function: ReLU\n  - Output shape: (1, 1, 128) [Calculation: (4 - 4) + 1 = 1]\n\n- Flatten layer:\n  - Reshapes the output to a 1D array of size 128.\n\n- Dense layer 1:\n  - Number of neurons: 512\n  - Activation function: ReLU\n  - Output shape: 512\n\n- Dropout layer:\n  - Dropout rate: 0.5\n  - Output shape: 512\n\n- Dense layer 2:\n  - Number of neurons: N_TYPES (the number of output classes)\n  - Activation function: Softmax\n  - Output shape: N_TYPES\n\nTotal trainable parameters: 495,972 (1.89 MB)","metadata":{}},{"cell_type":"markdown","source":"***\n<a name='CNN2'>\n    \n# 8 <span style='color:blue'>|</span> Final CNN Model\n    \nHere, we present the second version of our Convolutional Neural Network (CNN) architecture, which includes some important tweaks and additional modifications for improved performance. We have tested `BatchNormalization()` layers, which proved to be not so helpful. We also tested changing certain filter sizes such as the last `filter_size = (3,3)`. This change proved to have near no affect on the model's improvment on test accuracy.\n\nFurthermore, we performed fine-tuning on certain hyperparameters, specifically $\\beta_1$ and $\\beta_2$, which are part of the Adam optimizer. By experimenting with different values, we explored a range of $\\beta_1 \\in [0.7, 0.995]$ and $\\beta_2 \\in [0.9, 0.9995]$ to optimize the training process. After conducting an exhaustive evaluation of multiple models, we achieved the highest validation accuracy with the following configuration:\n\n**Adam Parameters:**\n- `learning_rate`: The learning rate determines the step size for adjusting the model weights during training. Higher values can lead to faster convergence, but they may also risk overshooting. The default value is 0.001.\n- `beta_1`: The exponential decay rate for the first moment estimates, controlling the exponential decay of the moving average of past gradients. The default value is 0.9.\n- `beta_2`: The exponential decay rate for the second moment estimates, controlling the exponential decay of the moving average of past squared gradients. The default value is 0.999.\n- `epsilon`: A small value added to the denominator for numerical stability. The default is 1e-7.\n- `decay`: This parameter gradually decreases the learning rate over time to fine-tune the model.\n- `amsgrad`: A boolean value indicating whether to use the AMSGrad variant of the Adam optimizer. The default is False.\n- `clipnorm`: Caps the norm of the gradients to a specified maximum value. It provides an alternative to `clipvalue`.\n- `clipvalue`: Prevents gradients from becoming too large by capping them at a specified maximum value.\n\nAdditionally, to enhance the training process and prevent overfitting, we implemented the following callbacks:\n\n***\n```python\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nEarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False, start_from_epoch=0)\nReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n```\n***\n    \nWith these callbacks, the model's training will stop early if the loss stops decreasing (using `EarlyStopping`), and the learning rate will be reduced if the validation loss plateaus (using `ReduceLROnPlateau`).\n\nThese additional tweaks have improved the performance of our CNN model. Through experimentation and fine-tuning, we have achieved better convergence and higher validation accuracy.","metadata":{}},{"cell_type":"code","source":"# Define the model architecture\nmodel = models.Sequential([\n    \n    # Convolutional layer 1\n    Conv2D(32, (4, 4), activation=\"relu\", input_shape=image_shape),\n    MaxPooling2D(pool_size=(3, 3)),\n\n    # Convolutional layer 2\n    Conv2D(64, (4, 4), activation=\"relu\"),\n    MaxPooling2D(pool_size=(3, 3)),\n\n    # Convolutional layer 3\n    Conv2D(128, (4, 4), activation=\"relu\"),\n    MaxPooling2D(pool_size=(3, 3)),\n\n    # Convolutional layer 4\n    Conv2D(128, (4, 4), activation=\"relu\"),\n    Flatten(),\n\n    # Full connect layers\n    Dense(512, activation=\"relu\"),\n    Dropout(0.5, seed=SEED),\n    Dense(N_TYPES, activation=\"softmax\")\n])\n\nmodel.summary()\n\noptimizer = legacy.Adam(learning_rate=0.001, beta_1=0.869, beta_2=0.995)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics= ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:54:17.710607Z","iopub.execute_input":"2023-12-06T22:54:17.711085Z","iopub.status.idle":"2023-12-06T22:54:17.892812Z","shell.execute_reply.started":"2023-12-06T22:54:17.711048Z","shell.execute_reply":"2023-12-06T22:54:17.891527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install visualkeras","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-06T22:54:20.638991Z","iopub.execute_input":"2023-12-06T22:54:20.639446Z","iopub.status.idle":"2023-12-06T22:54:35.344292Z","shell.execute_reply.started":"2023-12-06T22:54:20.639412Z","shell.execute_reply":"2023-12-06T22:54:35.343078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from visualkeras import layered_view\n\n# Visualize the model\nlayered_view(model, legend=True, max_xy=300)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:54:54.479906Z","iopub.execute_input":"2023-12-06T22:54:54.480375Z","iopub.status.idle":"2023-12-06T22:54:54.516179Z","shell.execute_reply.started":"2023-12-06T22:54:54.480337Z","shell.execute_reply":"2023-12-06T22:54:54.514871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_visual = models.Model(inputs=model.input, outputs=model.output)\n\n# Save model architecture to a file\nplot_model(model_visual, show_dtype=True, to_file='model_architecture.png', show_shapes=True)\n\n# Display model architecture in the notebook\n\nfrom IPython.display import Image\nImage(retina=True, filename='model_architecture.png')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T22:55:03.14036Z","iopub.execute_input":"2023-12-06T22:55:03.140786Z","iopub.status.idle":"2023-12-06T22:55:03.506433Z","shell.execute_reply.started":"2023-12-06T22:55:03.140754Z","shell.execute_reply":"2023-12-06T22:55:03.505078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> 8.1 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'>  Training Model <b>","metadata":{}},{"cell_type":"code","source":"# Stop training if loss doesn't keep decreasing.\nmodel_es = EarlyStopping(monitor='loss', min_delta=1e-9, patience=8, verbose=True)\nmodel_rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, verbose=True)\n\n# Training the model\nhistory = model.fit(train_generator,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=epochs,\n                    validation_data=test_generator,\n                    validation_steps=validation_steps,\n                    callbacks=[model_es, model_rlr])\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-12-06T22:55:06.495383Z","iopub.execute_input":"2023-12-06T22:55:06.495803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> 8.2 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'>  Model Evaluation <b>","metadata":{}},{"cell_type":"code","source":"# Evaluating the model\nloss, accuracy = model.evaluate(test_generator, steps=test_generator.samples//batch_size)\nprint(f\"Test Loss: {loss:0.5f}\")\nprint(f\"Test Accuracy: {accuracy:0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:12:56.58001Z","iopub.execute_input":"2023-07-28T12:12:56.580509Z","iopub.status.idle":"2023-07-28T12:13:06.96152Z","shell.execute_reply.started":"2023-07-28T12:12:56.580469Z","shell.execute_reply":"2023-07-28T12:13:06.960262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(ncols=2, figsize=(15, 6))\n\n# Plot the training and validation accuracy over epochs\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Model 2 Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('Accuracy')\nax[0].legend(['Train', 'Validation'])\nax[0].grid(alpha=0.2)\n\n# Plot the training and validation loss over epochs\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Model 2 Loss')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Loss')\nax[1].legend(['Train', 'Validation'])\nax[1].grid(alpha=0.2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:06.96304Z","iopub.execute_input":"2023-07-28T12:13:06.963639Z","iopub.status.idle":"2023-07-28T12:13:07.843631Z","shell.execute_reply.started":"2023-07-28T12:13:06.963604Z","shell.execute_reply":"2023-07-28T12:13:07.842354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting confusion matrix\nconfusion_matrix = CM(CNN_model=model, test_generator=test_generator, categories=class_indices_train_list)\n\nplt.figure(figsize=(8,8))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.xticks(ticks=np.arange(N_TYPES) + 0.5,\n           labels=[name.title() for name in class_indices_train_list], ha='center')\nplt.yticks(ticks=np.arange(N_TYPES) + 0.5, \n           labels=[name.title() for name in class_indices_train_list], va='center')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:07.845263Z","iopub.execute_input":"2023-07-28T12:13:07.845786Z","iopub.status.idle":"2023-07-28T12:13:18.688229Z","shell.execute_reply.started":"2023-07-28T12:13:07.845749Z","shell.execute_reply":"2023-07-28T12:13:18.687013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing metrics\ncalculate_metrics(confusion_matrix, categories=class_indices_train_list)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:12:27.200964Z","iopub.execute_input":"2023-07-28T12:12:27.201464Z","iopub.status.idle":"2023-07-28T12:12:27.208199Z","shell.execute_reply.started":"2023-07-28T12:12:27.201429Z","shell.execute_reply":"2023-07-28T12:12:27.207229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using functions in 6.1 for showing results\nplot_sample_predictions(model=model, \n                        test_generator=test_generator, \n                        categories=class_indices_train_list,\n                        test_dir=test_dir, \n                        num_samples=9,\n                        figsize=(13, 12))","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:12:27.209653Z","iopub.execute_input":"2023-07-28T12:12:27.210302Z","iopub.status.idle":"2023-07-28T12:12:38.096491Z","shell.execute_reply.started":"2023-07-28T12:12:27.210266Z","shell.execute_reply":"2023-07-28T12:12:38.095376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b> 8.4 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'>  Deeper Look Into Model 2 <b>","metadata":{}},{"cell_type":"code","source":"# channnel map plot\ndef plot_channel_activation_maps(model, image, images_per_row=16, N=8, save=False):\n    \"\"\"\n    Function to visualize how the first N layers of the model observe the input image.\n\n    Parameters:\n        model (tensorflow.keras.models.Model): The Keras model for which to visualize the activation maps.\n        image (numpy.ndarray): The input image for which to generate activation maps.\n        images_per_row (int): Number of activation maps to display per row in the grid.\n        N (int): Number of layers to visualize.\n        save (bool): If True, save the plots as PDF files.\n\n    Returns:\n        None\n    \"\"\"\n    from tensorflow.keras.models import Model\n    \n    # Create a sub-model that outputs activations for the first N layers\n    activation_model = Model(inputs=model.input, outputs=[layer.output for layer in model.layers[:N]])\n    activations = activation_model.predict(image)\n\n    # Get the names of the layers for labeling the plots\n    layer_names = [layer.name for layer in model.layers[:N]]\n\n    # Visualize the feature maps for each layer\n    for layer_name, layer_activation in zip(layer_names, activations):\n        # This is the number of features in the feature map\n        n_features = layer_activation.shape[-1]\n        # The feature map has shape (1, size, size, n_features)\n        size = layer_activation.shape[1]\n        # We will tile the activation channels in this matrix\n        n_cols = n_features // images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        # We'll tile each filter into this big horizontal grid\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n                # Post-process the feature to make it visually palatable\n                channel_image -= channel_image.mean()\n                epsilon = 1e-8  # A small epsilon value to avoid division by zero\n                channel_std = channel_image.std() + epsilon\n                channel_image /= channel_std\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size: (col + 1) * size,\n                             row * size: (row + 1) * size] = channel_image\n\n        # Display the grid\n        scale = 1. / size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.axis('off')\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n        if save:\n            plt.savefig(f'plot_channel_activation_maps_{layer_name}.pdf')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:23.214816Z","iopub.execute_input":"2023-07-28T12:13:23.216265Z","iopub.status.idle":"2023-07-28T12:13:23.233089Z","shell.execute_reply.started":"2023-07-28T12:13:23.216221Z","shell.execute_reply":"2023-07-28T12:13:23.231601Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the next batch from the test generator\nbatch_images, batch_labels = next(test_generator)\n\n# Extract the first image from the batch\nimage, label = batch_images[0], batch_labels[0]\nimage_tensor = np.expand_dims(image, axis=0)\n\n# Get the class indices from the test generator\nclass_indices = test_generator.class_indices\n\n# Convert the one-hot encoded label to the class name\nlabel_name = [k for k, v in class_indices.items() if np.argmax(label) == v][0]\n\n# Display the class name\nprint(f\"Class name of the first image: {label_name}\")\nprint(f'Shape {image_tensor.shape}')\narray_to_img(image_tensor[0])","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:23.489113Z","iopub.execute_input":"2023-07-28T12:13:23.489692Z","iopub.status.idle":"2023-07-28T12:13:23.602698Z","shell.execute_reply.started":"2023-07-28T12:13:23.489649Z","shell.execute_reply":"2023-07-28T12:13:23.601478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_channel_activation_maps(model=model, image=image_tensor, N=5, save=SAVE)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:24.660745Z","iopub.execute_input":"2023-07-28T12:13:24.661837Z","iopub.status.idle":"2023-07-28T12:13:27.287978Z","shell.execute_reply.started":"2023-07-28T12:13:24.661772Z","shell.execute_reply":"2023-07-28T12:13:27.286748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b> 8.4.2 <span style='border-left: 4px solid #0000FF; padding-left: 10px;'> Misclassified Tumors <b>","metadata":{}},{"cell_type":"code","source":"# Visualization of mis-classsified images\ndef visualize_misclassified_images(model, test_generator, class_indices):\n    \"\"\"\n    Visualize misclassified images from the test set alongside their predicted and true labels.\n\n    Parameters:\n        model (tensorflow.keras.models.Model): The trained Keras model.\n        test_generator (tensorflow.keras.preprocessing.image.DirectoryIterator): The test data generator.\n        class_indices (dict): Dictionary mapping class names to their corresponding integer labels.\n\n    Returns:\n        None\n    \"\"\"\n    \n    from tensorflow.keras.preprocessing.image import array_to_img\n    \n    misclassified_images = []\n    misclassified_labels_true = []\n    misclassified_labels_pred = []\n\n    for i in range(len(test_generator)):\n        batch_images, batch_labels = next(test_generator)\n        batch_predictions = model.predict(batch_images, verbose=False)\n        predicted_labels = [list(class_indices.keys())[np.argmax(pred)] for pred in batch_predictions]\n        true_labels = [list(class_indices.keys())[np.argmax(label)] for label in batch_labels]\n\n        for j in range(len(batch_images)):\n            if predicted_labels[j] != true_labels[j]:\n                misclassified_images.append(batch_images[j])\n                misclassified_labels_true.append(true_labels[j])\n                misclassified_labels_pred.append(predicted_labels[j])\n\n    # Display misclassified images alongside their true and predicted labels\n    num_misclassified = len(misclassified_images)\n    num_rows = int(np.ceil(num_misclassified / 4))\n    plt.figure(figsize=(12, 3 * num_rows))\n\n    for i in range(num_misclassified):\n        plt.subplot(num_rows, 4, i + 1)\n        plt.title(f\"True: {misclassified_labels_true[i]}\\nPred: {misclassified_labels_pred[i]}\", color='red')\n        plt.imshow(array_to_img(misclassified_images[i]))\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:29.409105Z","iopub.execute_input":"2023-07-28T12:13:29.409965Z","iopub.status.idle":"2023-07-28T12:13:29.422653Z","shell.execute_reply.started":"2023-07-28T12:13:29.409917Z","shell.execute_reply":"2023-07-28T12:13:29.421591Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using function above \nvisualize_misclassified_images(model, test_generator, test_generator.class_indices)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T12:13:29.623402Z","iopub.execute_input":"2023-07-28T12:13:29.626235Z","iopub.status.idle":"2023-07-28T12:13:43.692466Z","shell.execute_reply.started":"2023-07-28T12:13:29.626189Z","shell.execute_reply":"2023-07-28T12:13:43.691097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='end'>\n    \n***\n0. [`Return To Top of Notebook`](#T)\n1. [`About The Data`](#data)\n2. [`Imports & Setup`](#imports)\n3. [`Imports Data`](#import_data)\n4. [`Data Visualization`](#vis)\n5. [`Data Processing`](#DP)\n6. [`Analysis of CNN Output`](#a_cnn)\n7. [`First CNN`](#CNN)\n8. [`Second CNN`](#CNN2)\n9. [`End of Notebook`](#end)\n\nAuthor: [Math & Physics Fun with Gus](https://mathphysicsfunwithgus.square.site)\n\n<p style=\"padding: 10px;\n          background-color: yellow;\n          font-family: computermodern;\n          color: black;\n          font-size: 210%;\n          text-align: center;\n          border-radius:20px 20px;\n          \">Thank you and if you found this useful please like üëçüèº </p>\n    \n# <b> END <span style='border-left: 4px solid #0000FF; padding-left: 10px;'>  Jupyter Notebook <b>\n***","metadata":{}}]}